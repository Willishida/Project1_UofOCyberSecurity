Question Selection: “Challenges collecting a large amount of log data.”


	The issue of collecting large amounts of log is a real challenge. Depending on the size of the organization and how much traffic is generated, there can be a great deal of log data. For instance, an organization such as Facebook may have hundreds of million unique visitors interacting with their web servers on a daily basis. While Facebook is an extreme example, the issue remains the same, users interacting with an organization’s web servers generates a great deal of traffic, even for simple transactions. Best practice would dictate that all of this information be stored and retained at least for some period of time for security purposes.

	In this project I setup two web servers as well as an ELK stack in order to monitor the web traffic. Without ELK or a similar utility, managing, organizing and reviewing logs generated by web traffic would be impossible, there is simply too much data. For this project, I primarily used Kibana sample data as my web servers were not setup for a large amount of web traffic. The sample data provided by Kibana simulated a small to moderate amount of traffic. I was primarily investigating from where users were interacting with the web server, when they were interacting with the web server as well as what they were doing while interacting with the web server. 

	Kibana was the tool that allowed me to parse through the log data. Just having an aggregation of the logs wouldn’t have been very useful. Going through each packet individually would be incredibly impractical. Kibana’s ability to search a great deal of data is what makes log review possible. Beyond Kibana, I primarily needed an understanding of HTTP networking to find the information I was looking for. It was very important to understand how HTTP traffic is transmitted and what the packets contain. Knowing that I can filter Kibana by geographical source, IP address, HTTP response, etc. proved critical.

	I primarily used the search function within Kibana to find the information I needed to find. While the visualizations are great for understanding what is generally happening with the web traffic, searching for specific information, as I had to do for this project, is much more precise. For instance the query “response:404” can show me the exact number of 404 responses were given within the specified time period. I applied this search method to all of the data I was tasked with finding.

	The data collected (provided by Kibana sample data) proved to be sufficient for what I was looking for. The traffic logged was fairly standard and had all of the information needed. Finding country of origin, response codes, information on OS’s, etc. was all available. Having more data probably would not have changed my conclusion as the data provided was sufficient.

